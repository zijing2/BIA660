{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############## label CNN performance is as follows: ##############\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  amenities       1.00      0.71      0.83        48\n",
      "environment       0.97      0.76      0.85        41\n",
      "       food       0.98      0.88      0.93       180\n",
      "   location       1.00      0.81      0.89        26\n",
      "       null       0.99      0.87      0.93       136\n",
      "      price       1.00      0.82      0.90        40\n",
      "    service       0.99      0.81      0.89       102\n",
      "\n",
      "avg / total       0.99      0.84      0.90       573\n",
      "\n",
      "\n",
      "############## sentiment CNN performance is as follows: ##############\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.98      0.96       363\n",
      "          1       0.93      0.86      0.89       137\n",
      "\n",
      "avg / total       0.94      0.94      0.94       500\n",
      "\n",
      "\n",
      "############## quality ANN performance is as follows: ##############\n",
      "\n",
      "{'label_predict': {'the beef steak is crispy  and juicy.': {'service': 0.005936117842793465, 'food': 0.9989669322967529, 'price': 0.015411414206027985, 'environment': 0.0036307305563241243, 'amenities': 0.010868694633245468, 'location': 0.0015943213365972042, 'null': 0.0029294854030013084}, 'the beer is cold': {'service': 0.022446535527706146, 'food': 0.8835346102714539, 'price': 0.013226679526269436, 'environment': 0.021535640582442284, 'amenities': 0.11269669234752655, 'location': 0.010962963104248047, 'null': 0.025443796068429947}, 'the beef steak is good.': {'service': 0.000801258604042232, 'food': 0.9970982074737549, 'price': 0.007220506202429533, 'environment': 0.0008686440996825695, 'amenities': 0.00103157723788172, 'location': 0.0010762664023786783, 'null': 0.030904270708560944}, 'the beer is good too.': {'service': 0.0029942390974611044, 'food': 0.5981521010398865, 'price': 0.018451161682605743, 'environment': 0.009218058548867702, 'amenities': 0.00982755422592163, 'location': 0.008505474776029587, 'null': 0.20521657168865204}}, 'sentiment_predict': {'the beef steak is crispy  and juicy.': 0.20569553971290588, 'the beer is cold': 0.21960698068141937, 'the beef steak is good.': 0.6330832242965698, 'the beer is good too.': 0.822425127029419}, 'review_predict': [0.6158599257469177, 0.567529559135437]}\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import json\n",
    "import gensim\n",
    "import nltk,string\n",
    "from random import shuffle\n",
    "from gensim.models import doc2vec\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, \\\n",
    "Dropout, Activation, Input, Flatten, Concatenate\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from nltk import tokenize\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import matplotlib  \n",
    "matplotlib.use('Agg') \n",
    "from matplotlib.pyplot import plot,savefig \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn import metrics\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "\n",
    "DOCVECTOR_MODEL=\"docvector_model\"\n",
    "BEST_MODEL_FILEPATH=\"best_model\"\n",
    "BEST_SENT_MODEL_FILEPATH=\"best_sent_model\"\n",
    "QUALITY_MODEL=\"quality_model\"\n",
    "MAX_NB_WORDS=1467\n",
    "MAX_DOC_LEN=200\n",
    "EMBEDDING_DIM=200\n",
    "FILTER_SIZES=[2,3,4]\n",
    "BTACH_SIZE = 64\n",
    "NUM_EPOCHES = 40\n",
    "\n",
    "class ReviewAnalyser(object):\n",
    "    \n",
    "    # review's ann model\n",
    "    ann_model = None\n",
    "    # label's cnn model\n",
    "    label_model = None\n",
    "    # label's classification: ['amenities' 'environment' 'food' 'location' 'null' 'price' 'service' 'transport']\n",
    "    label_mlb = None\n",
    "    # labels input padding sequence\n",
    "    label_padding_sequence = None\n",
    "    # labels actual classification\n",
    "    label_act = None\n",
    "    # sentiment's cnn model\n",
    "    sent_model = None\n",
    "    # sentiment's classification: ['0', '1'] 0: neutral, 1: positive/negative\n",
    "    sent_mlb = None\n",
    "    # sentiment input padding sequence\n",
    "    sent_padding_sequence = None\n",
    "    # sentiment actual classification\n",
    "    sent_act = None\n",
    "    # doc2vector's cnn model\n",
    "    wv_model = None\n",
    "    \n",
    "    def __init__(self, data): \n",
    "        self.data = data;\n",
    "        \n",
    "    @staticmethod\n",
    "    def ann_model():\n",
    "        lam=0.01\n",
    "        model = Sequential()\n",
    "        model.add(Dense(12, input_dim=8, activation='relu', \\\n",
    "                        kernel_regularizer=l2(lam), name='L2') )\n",
    "        model.add(Dense(8, activation='relu', \\\n",
    "                        kernel_regularizer=l2(lam),name='L3') )\n",
    "        model.add(Dense(1, activation='sigmoid', name='Output'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "        \n",
    "    @staticmethod    \n",
    "    def cnn_model(FILTER_SIZES, \\\n",
    "        # filter sizes as a list\n",
    "        MAX_NB_WORDS, \\\n",
    "        # total number of words\n",
    "        MAX_DOC_LEN, \\\n",
    "        # max words in a doc\n",
    "        NUM_OUTPUT_UNITS=1, \\\n",
    "        # number of output units\n",
    "        EMBEDDING_DIM=200, \\\n",
    "        # word vector dimension\n",
    "        NUM_FILTERS=64, \\\n",
    "        # number of filters for all size\n",
    "        DROP_OUT=0.5, \\\n",
    "        # dropout rate\n",
    "        PRETRAINED_WORD_VECTOR=None,\\\n",
    "        # Whether to use pretrained word vectors\n",
    "        LAM=0.01,\\\n",
    "        ACTIVATION='sigmoid'):            \n",
    "        # regularization coefficient\n",
    "    \n",
    "        main_input = Input(shape=(MAX_DOC_LEN,), \\\n",
    "                           dtype='int32', name='main_input')\n",
    "\n",
    "        if PRETRAINED_WORD_VECTOR is not None:\n",
    "            embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                            output_dim=EMBEDDING_DIM, \\\n",
    "                            input_length=MAX_DOC_LEN, \\\n",
    "                            weights=[PRETRAINED_WORD_VECTOR],\\\n",
    "                            trainable=False,\\\n",
    "                            name='embedding')(main_input)\n",
    "        else:\n",
    "            embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                            output_dim=EMBEDDING_DIM, \\\n",
    "                            input_length=MAX_DOC_LEN, \\\n",
    "                            name='embedding')(main_input)\n",
    "\n",
    "        conv_blocks = []\n",
    "        for f in FILTER_SIZES:\n",
    "            conv = Conv1D(filters=NUM_FILTERS, kernel_size=f, \\\n",
    "                          activation='relu', name='conv_'+str(f))(embed_1)\n",
    "            conv = MaxPooling1D(MAX_DOC_LEN-f+1, name='max_'+str(f))(conv)\n",
    "            conv = Flatten(name='flat_'+str(f))(conv)\n",
    "            conv_blocks.append(conv)\n",
    "\n",
    "        z=Concatenate(name='concate')(conv_blocks)\n",
    "        drop=Dropout(rate=DROP_OUT, name='dropout')(z)\n",
    "\n",
    "        dense = Dense(192, activation='relu',\\\n",
    "                        kernel_regularizer=l2(LAM),name='dense')(drop)\n",
    "        preds = Dense(NUM_OUTPUT_UNITS, activation=ACTIVATION, name='output')(dense)\n",
    "        model = Model(inputs=main_input, outputs=preds)\n",
    "\n",
    "        model.compile(loss=\"binary_crossentropy\", \\\n",
    "                  optimizer=\"adam\", metrics=[\"accuracy\"]) \n",
    "\n",
    "        return model\n",
    "\n",
    "    # training to change document into vector using gensim\n",
    "    def pretrain(self, RETRAIN=0):\n",
    "        with open(\"word_sample.json\", 'r') as f:\n",
    "            reviews=[]\n",
    "            for line in f: \n",
    "                review = json.loads(line) \n",
    "                try:\n",
    "                    review[\"text\"].strip().lower().encode('ascII')\n",
    "                except:\n",
    "                    # do nothing\n",
    "                    a = 1\n",
    "                else:\n",
    "                    reviews.append(review[\"text\"])\n",
    "\n",
    "        sentences=[ [token.strip(string.punctuation).strip() \\\n",
    "                     for token in nltk.word_tokenize(doc.lower()) \\\n",
    "                         if token not in string.punctuation and \\\n",
    "                         len(token.strip(string.punctuation).strip())>=2]\\\n",
    "                     for doc in reviews]\n",
    "\n",
    "\n",
    "        docs=[TaggedDocument(sentences[i], [str(i)]) for i in range(len(sentences)) ]\n",
    "        \n",
    "        if RETRAIN==0 and os.path.exists(DOCVECTOR_MODEL):\n",
    "            self.wv_model = doc2vec.Doc2Vec.load(DOCVECTOR_MODEL)\n",
    "        else:\n",
    "            self.wv_model = doc2vec.Doc2Vec(dm=1, min_count=5, window=5, size=200, workers=4)\n",
    "            self.wv_model.build_vocab(docs)\n",
    "            for epoch in range(30):\n",
    "                # shuffle the documents in each epoch\n",
    "                shuffle(docs)\n",
    "                # in each epoch, all samples are used\n",
    "                self.wv_model.train(docs, total_examples=len(docs), epochs=1)\n",
    "                \n",
    "            self.wv_model.save(DOCVECTOR_MODEL)\n",
    "\n",
    "#         print(\"Top 5 words similar to word 'price'\")\n",
    "#         print self.wv_model.wv.most_similar('price', topn=5)\n",
    "\n",
    "#         print(\"Top 5 words similar to word 'price' but not relevant to 'bathroom'\")\n",
    "#         print self.wv_model.wv.most_similar(positive=['price','money'], negative=['bathroom'], topn=5)\n",
    "\n",
    "#         print(\"Similarity between 'price' and 'bathroom':\")\n",
    "#         print self.wv_model.wv.similarity('price','bathroom') \n",
    "\n",
    "#         print(\"Similarity between 'price' and 'charge':\")\n",
    "#         print self.wv_model.wv.similarity('price','charge') \n",
    "\n",
    "#         print self.wv_model.wv\n",
    "\n",
    "    # training labels CNN\n",
    "    def trainLebels(self, RETRAIN=0):\n",
    "        labels = []\n",
    "        # fetch labels for each sentence        \n",
    "        for subdata in self.data[2][0:500]:\n",
    "            label = []\n",
    "            for d in subdata.split(\",\"):\n",
    "                label.append(d.strip())\n",
    "            labels.append(label)\n",
    "            \n",
    "        mlb = MultiLabelBinarizer()\n",
    "        Y=mlb.fit_transform(labels)\n",
    "        self.label_act = Y\n",
    "        self.label_mlb = mlb\n",
    "        np.sum(Y, axis=0)\n",
    "\n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(self.data[1][0:500])\n",
    "        NUM_WORDS = min(MAX_NB_WORDS, len(tokenizer.word_index))\n",
    "        embedding_matrix = np.zeros((NUM_WORDS+1, EMBEDDING_DIM))\n",
    "\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            if i >= NUM_WORDS:\n",
    "                continue\n",
    "            if word in self.wv_model.wv:\n",
    "                embedding_matrix[i]=self.wv_model.wv[word]\n",
    "\n",
    "        voc=tokenizer.word_index\n",
    "        sequences = tokenizer.texts_to_sequences(self.data[1][0:500])\n",
    "        padded_sequences = pad_sequences(sequences, \\\n",
    "                                         maxlen=MAX_DOC_LEN, \\\n",
    "                                         padding='post', truncating='post')\n",
    "        self.label_padding_sequence = padded_sequences\n",
    "        \n",
    "\n",
    "        NUM_OUTPUT_UNITS=len(mlb.classes_)\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\\\n",
    "                        padded_sequences, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "        if(RETRAIN == 0 and os.path.exists(BEST_MODEL_FILEPATH)):\n",
    "#                 self.label_model.load_weights(BEST_MODEL_FILEPATH)\n",
    "                self.label_model = load_model(BEST_MODEL_FILEPATH)\n",
    "                pred=self.label_model.predict(padded_sequences[0:500])\n",
    "                return\n",
    "        \n",
    "        self.label_model=ReviewAnalyser.cnn_model(FILTER_SIZES, MAX_NB_WORDS, \\\n",
    "                        MAX_DOC_LEN, NUM_OUTPUT_UNITS, \\\n",
    "                        PRETRAINED_WORD_VECTOR=embedding_matrix)\n",
    "\n",
    "        earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=2, mode='min')\n",
    "        checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH, monitor='val_acc', \\\n",
    "                                     verbose=2, save_best_only=True, mode='max')\n",
    "        \n",
    "        training=self.label_model.fit(X_train, Y_train, \\\n",
    "                  batch_size=BTACH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "                  callbacks=[earlyStopping, checkpoint],\\\n",
    "                  validation_data=[X_test, Y_test], verbose=2)\n",
    "        \n",
    "        self.label_model.save(BEST_MODEL_FILEPATH)\n",
    "        \n",
    "        return\n",
    "        \n",
    "    # training sentiment CNN        \n",
    "    def trainSentiment(self, RETRAIN=0):\n",
    "        labels = []\n",
    "        for i,subdata in enumerate(self.data[3][0:500]):\n",
    "            if subdata == 1:\n",
    "                labels.append(['1'])\n",
    "            else:\n",
    "                labels.append(['0'])\n",
    "\n",
    "        Y_labels = np.copy(labels)\n",
    "        mlb = LabelBinarizer()\n",
    "        Y = mlb.fit_transform(Y_labels)\n",
    "        self.sent_act = Y\n",
    "        self.sent_mlb = mlb\n",
    "        \n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(self.data[1][0:500])\n",
    "        NUM_WORDS = min(MAX_NB_WORDS, len(tokenizer.word_index))\n",
    "        embedding_matrix = np.zeros((NUM_WORDS+1, EMBEDDING_DIM))\n",
    "\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            if i >= NUM_WORDS:\n",
    "                continue\n",
    "            if word in self.wv_model.wv:\n",
    "                embedding_matrix[i]=self.wv_model.wv[word]\n",
    "\n",
    "        voc=tokenizer.word_index\n",
    "        sequences = tokenizer.texts_to_sequences(self.data[1][0:500])\n",
    "        padded_sequences = pad_sequences(sequences, \\\n",
    "                                         maxlen=MAX_DOC_LEN, \\\n",
    "                                         padding='post', truncating='post')\n",
    "        self.sent_padding_sequence = padded_sequences\n",
    "\n",
    "        NUM_OUTPUT_UNITS=len(mlb.classes_)\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(padded_sequences, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "        \n",
    "        if(RETRAIN == 0 and os.path.exists(BEST_SENT_MODEL_FILEPATH)):\n",
    "#                 self.sent_model.load_weights(\"best_sent_model\")\n",
    "                self.sent_model = load_model(BEST_SENT_MODEL_FILEPATH)\n",
    "                pred=self.sent_model.predict(padded_sequences[0:500])\n",
    "                return\n",
    "        \n",
    "        self.sent_model=ReviewAnalyser.cnn_model(FILTER_SIZES, MAX_NB_WORDS, \\\n",
    "                    MAX_DOC_LEN, \\\n",
    "                    PRETRAINED_WORD_VECTOR=embedding_matrix)\n",
    "\n",
    "        earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=2, mode='min')\n",
    "        checkpoint = ModelCheckpoint(BEST_SENT_MODEL_FILEPATH, monitor='val_acc', \\\n",
    "                                     verbose=2, save_best_only=True, mode='max')\n",
    "\n",
    "        training=self.sent_model.fit(X_train, Y_train, \\\n",
    "                  batch_size=BTACH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "                  callbacks=[earlyStopping, checkpoint],\\\n",
    "                  validation_data=[X_test, Y_test], verbose=2) \n",
    "        \n",
    "        self.sent_model.save(BEST_SENT_MODEL_FILEPATH)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    # training review quality ANN\n",
    "    def trainQuality(self, RETRAIN=0, PERFORMANCE=0):\n",
    "        rows = {}\n",
    "        for subdata in self.data[0:192].values.tolist():\n",
    "            if rows.has_key(subdata[0]):\n",
    "                labels = subdata[2].split(',')\n",
    "                for label in labels:\n",
    "                    rows[subdata[0]][label.strip()] = rows[subdata[0]][label.strip()]+1.0\n",
    "                rows[subdata[0]][\"sentiment\"] = rows[subdata[0]][\"sentiment\"] + subdata[3]\n",
    "                rows[subdata[0]][\"quality\"] = subdata[4]\n",
    "                rows[subdata[0]][\"items\"] = rows[subdata[0]][\"items\"] + 1\n",
    "            else:\n",
    "                rows[subdata[0]] = {\n",
    "                    'items' : 0.0,\n",
    "                    'amenities' : 0.0,\n",
    "                    'environment' : 0.0,\n",
    "                    'food' : 0.0,\n",
    "                    'location' : 0.0,\n",
    "                    'null' : 0.0,\n",
    "                    'price': 0.0,\n",
    "                    'service': 0.0,\n",
    "                    'sentiment': 0.0,\n",
    "                    'quality': 0.0\n",
    "                }\n",
    "        data = []\n",
    "        for key in rows:\n",
    "            subdata=[]\n",
    "            subdata.append(rows[key][\"amenities\"]/rows[key][\"items\"])\n",
    "            subdata.append(rows[key][\"environment\"]/rows[key][\"items\"])\n",
    "            subdata.append(rows[key][\"food\"]/rows[key][\"items\"])\n",
    "            subdata.append(rows[key][\"location\"]/rows[key][\"items\"])\n",
    "            subdata.append(rows[key][\"null\"]/rows[key][\"items\"])\n",
    "            subdata.append(rows[key][\"price\"]/rows[key][\"items\"])\n",
    "            subdata.append(rows[key][\"service\"]/rows[key][\"items\"])\n",
    "            subdata.append(rows[key][\"sentiment\"]/rows[key][\"items\"])\n",
    "            subdata.append(rows[key][\"quality\"])\n",
    "            data.append(subdata)\n",
    "\n",
    "        df=pd.DataFrame(data, columns=[\"amenities\",\"environment\",\"food\",\"location\",\"null\",\"price\",\"service\",\"sentiment\",\"quality\"])\n",
    "        X=df.values[:,0:8]\n",
    "        Y=df.values[:,8]\n",
    "\n",
    "        if RETRAIN == 0 and os.path.exists(QUALITY_MODEL):\n",
    "            self.ann_model = load_model(QUALITY_MODEL)\n",
    "        else:\n",
    "            self.ann_model = ReviewAnalyser.ann_model()\n",
    "            training=self.ann_model.fit(X, Y, validation_split=0.3, shuffle=True, epochs=150, batch_size=32, verbose=2)\n",
    "            self.ann_model.save(QUALITY_MODEL)\n",
    "        \n",
    "        if PERFORMANCE==1:\n",
    "            scores = self.ann_model.evaluate(X, Y)\n",
    "            print(\"\\n%s: %.2f%%\" % (self.ann_model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "            predicted=self.ann_model.predict(X)\n",
    "            predicted=np.reshape(predicted, -1)\n",
    "            predicted=np.where(predicted>0.5, 1, 0)\n",
    "            print(metrics.classification_report(Y, predicted, labels=[0,1]))\n",
    "        \n",
    "        \n",
    "        return \n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def checkPerform(model, mlb, data_tobe_predicted, Y_actual):\n",
    "        pred=model.predict(data_tobe_predicted)\n",
    "        Y_pred=np.copy(pred)\n",
    "        Y_pred=np.where(Y_pred>0.5,1,0)\n",
    "        print(classification_report(Y_actual, Y_pred, \\\n",
    "                                    target_names=mlb.classes_))\n",
    "        return classification_report(Y_actual, Y_pred, \\\n",
    "                                    target_names=mlb.classes_)\n",
    "       \n",
    "    # check document information to determine the value of hyper-parameter\n",
    "    def checkDocInform(self):  \n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(self.data[1][0:500])\n",
    "        total_nb_words=len(tokenizer.word_counts)\n",
    "        sequences = tokenizer.texts_to_sequences(self.data[1][0:500])\n",
    "        print \"\\n############## document information ##############\\n\"\n",
    "        print \"total_nb_words:\"\n",
    "        print(total_nb_words)\n",
    "\n",
    "        word_counts=pd.DataFrame(\\\n",
    "                    tokenizer.word_counts.items(), \\\n",
    "                    columns=['word','count'])\n",
    "        df=word_counts['count'].value_counts().reset_index()\n",
    "        df['percent']=df['count']/len(tokenizer.word_counts)\n",
    "        df['cumsum']=df['percent'].cumsum()\n",
    "\n",
    "        plt.bar(df[\"index\"].iloc[0:50], df[\"percent\"].iloc[0:50])\n",
    "        plt.plot(df[\"index\"].iloc[0:50], df['cumsum'].iloc[0:50], c='green')\n",
    "\n",
    "        plt.xlabel('Word Frequency')\n",
    "        plt.ylabel('Percentage')\n",
    "        savefig('1.jpg')\n",
    "        plt.show()\n",
    "        \n",
    "        sen_len=pd.Series([len(item) for item in sequences])\n",
    "\n",
    "        df=sen_len.value_counts().reset_index().sort_values(by='index')\n",
    "        df.columns=['index','counts']\n",
    "\n",
    "        df=df.sort_values(by='index')\n",
    "        df['percent']=df['counts']/len(sen_len)\n",
    "        df['cumsum']=df['percent'].cumsum()\n",
    "        \n",
    "        plt.plot(df[\"index\"], df['cumsum'], c='green')\n",
    "\n",
    "        plt.xlabel('Sentence Length')\n",
    "        plt.ylabel('Percentage')\n",
    "        savefig('2.jpg')\n",
    "        plt.show()\n",
    "        \n",
    "    # predict labels for text, need to execute trainLabels first\n",
    "    def predictLabels(self, text_arr=[]):\n",
    "        if len(text_arr)==0:\n",
    "            return\n",
    "        rtn = {}\n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(self.data[1][0:500])\n",
    "        sub_sequences = tokenizer.texts_to_sequences(text_arr)\n",
    "        padded_sub_sequences = pad_sequences(sub_sequences, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', truncating='post')\n",
    "        sub_pred = self.label_model.predict(padded_sub_sequences)\n",
    "        for i, key in enumerate(text_arr):\n",
    "            dict1 = {}\n",
    "            pred_list = sub_pred[i].tolist()\n",
    "            for i, sub_pred_list in enumerate(pred_list):\n",
    "                dict1[self.label_mlb.classes_[i]] = pred_list[i]\n",
    "            rtn[key] = dict1\n",
    "        return rtn\n",
    "        \n",
    "    # predict sentiments for text, need to execute trainSentiment first    \n",
    "    def predictSentiment(self, text_arr=[]):\n",
    "        if len(text_arr)==0:\n",
    "            return\n",
    "        rtn = {}\n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(self.data[1][0:500])\n",
    "        sub_sequences = tokenizer.texts_to_sequences(text_arr)\n",
    "        padded_sub_sequences = pad_sequences(sub_sequences, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', truncating='post')\n",
    "        sub_pred = self.sent_model.predict(padded_sub_sequences)\n",
    "        for i, key in enumerate(text_arr):\n",
    "            rtn[key] = sub_pred[i].tolist()[0]\n",
    "        return rtn\n",
    "    \n",
    "    # predict quality for reviews, need to execute trainLabels,trainSentiment and trainQuality first    \n",
    "    def predictQuality(self, review_arr=[]):\n",
    "        text_arr=[]\n",
    "        sentence_review_mapping = []\n",
    "        data = []\n",
    "        rows = []\n",
    "        if len(review_arr)==0:\n",
    "            return\n",
    "        for i, rev in enumerate(review_arr):\n",
    "            rows.append({\n",
    "                'items' : 0.0,\n",
    "                'amenities' : 0.0,\n",
    "                'environment' : 0.0,\n",
    "                'food' : 0.0,\n",
    "                'location' : 0.0,\n",
    "                'null' : 0.0,\n",
    "                'price': 0.0,\n",
    "                'service': 0.0,\n",
    "                'sentiment': 0.0\n",
    "            })\n",
    "            rev_sent = tokenize.sent_tokenize(rev)\n",
    "            for sent in rev_sent:\n",
    "                text_arr.append(sent)\n",
    "                sentence_review_mapping.append((i,sent))\n",
    "            \n",
    "        label_predict = self.predictLabels(text_arr)\n",
    "        sentiment_predict = self.predictSentiment(text_arr)\n",
    "#         print sentence_review_mapping\n",
    "       \n",
    "        for mapping in sentence_review_mapping:\n",
    "            rows[mapping[0]][\"items\"] = rows[mapping[0]][\"items\"] + 1\n",
    "            rows[mapping[0]][\"amenities\"] = rows[mapping[0]][\"amenities\"]+label_predict[mapping[1]][\"amenities\"] \n",
    "            rows[mapping[0]][\"environment\"] = rows[mapping[0]][\"environment\"]+label_predict[mapping[1]][\"environment\"] \n",
    "            rows[mapping[0]][\"food\"] = rows[mapping[0]][\"food\"]+label_predict[mapping[1]][\"food\"]\n",
    "            rows[mapping[0]][\"location\"] = rows[mapping[0]][\"location\"]+label_predict[mapping[1]][\"location\"]\n",
    "            rows[mapping[0]][\"null\"] = rows[mapping[0]][\"null\"]+label_predict[mapping[1]][\"null\"]\n",
    "            rows[mapping[0]][\"price\"] = rows[mapping[0]][\"price\"]+label_predict[mapping[1]][\"price\"]\n",
    "            rows[mapping[0]][\"service\"] = rows[mapping[0]][\"service\"]+label_predict[mapping[1]][\"service\"]\n",
    "            rows[mapping[0]][\"sentiment\"] = rows[mapping[0]][\"sentiment\"]+sentiment_predict[mapping[1]]\n",
    "            \n",
    "        data = []\n",
    "        for row in rows:\n",
    "            subdata=[]\n",
    "            subdata.append(row[\"amenities\"]/row[\"items\"])\n",
    "            subdata.append(row[\"environment\"]/row[\"items\"])\n",
    "            subdata.append(row[\"food\"]/row[\"items\"])\n",
    "            subdata.append(row[\"location\"]/row[\"items\"])\n",
    "            subdata.append(row[\"null\"]/row[\"items\"])\n",
    "            subdata.append(row[\"price\"]/row[\"items\"])\n",
    "            subdata.append(row[\"service\"]/row[\"items\"])\n",
    "            subdata.append(row[\"sentiment\"]/row[\"items\"])\n",
    "            data.append(subdata)\n",
    "        df=pd.DataFrame(data, columns=[\"amenities\",\"environment\",\"food\",\"location\",\"null\",\"price\",\"service\",\"sentiment\"])\n",
    "        X = df.values[:,0:8]\n",
    "        \n",
    "        predicted=self.ann_model.predict(X)\n",
    "        predicted=np.reshape(predicted, -1)\n",
    "#         print(predicted)\n",
    "        #predicted=np.where(predicted>0.5, 1, 0)\n",
    "        rtn = {\n",
    "            \"label_predict\": label_predict,\n",
    "            \"sentiment_predict\": sentiment_predict,\n",
    "            \"review_predict\": predicted.tolist()\n",
    "        }\n",
    "        return rtn\n",
    "\n",
    "data=pd.read_csv(\"data_sample2.csv\",header=None)\n",
    "ra = ReviewAnalyser(data)\n",
    "ra.pretrain(RETRAIN=0)\n",
    "# ra.checkDocInform()\n",
    "ra.trainLebels(RETRAIN=0)\n",
    "print \"\\n############## label CNN performance is as follows: ##############\\n\"\n",
    "ReviewAnalyser.checkPerform(ra.label_model, ra.label_mlb, ra.label_padding_sequence, ra.label_act)\n",
    "ra.trainSentiment(RETRAIN=0)\n",
    "print \"\\n############## sentiment CNN performance is as follows: ##############\\n\"\n",
    "ReviewAnalyser.checkPerform(ra.sent_model, ra.sent_mlb, ra.sent_padding_sequence, ra.sent_act)\n",
    "# label_predict = ra.predictLabels(text_arr=[\"the burger is good\", \"the staff is nice\"])\n",
    "# print label_predict\n",
    "# sentiment_predict = ra.predictSentiment(text_arr=[\"the burger is good\", \"the staff is nice\"])\n",
    "# print sentiment_predict\n",
    "print \"\\n############## quality ANN performance is as follows: ##############\\n\"\n",
    "ra.trainQuality(RETRAIN=0)\n",
    "print ra.predictQuality(review_arr=[\"the beef steak is good. the beer is good too.\", \"the beef steak is crispy  and juicy. the beer is cold\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
